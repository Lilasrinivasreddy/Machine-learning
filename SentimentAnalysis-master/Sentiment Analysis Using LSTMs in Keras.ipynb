{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "//anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Original Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('stanford_movie_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = data['review'].iloc[0:25000]\n",
    "y_train = data['sentiment'].iloc[0:25000]\n",
    "\n",
    "X_test = data['review'].iloc[25000:]\n",
    "y_test = data['sentiment'].iloc[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(imdb.get_word_index().values())) == len(imdb.get_word_index().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re # regex library\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # Effectively removes HTML markup tags\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['review'] = data['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103893"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_int_sequence(text):\n",
    "    return [tokenizer.word_index[word] for word in text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "class LSTM_Sentiment_Classifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, embedding_vector_length, max_seq_length, lstm_layers, batch_size=32, num_epochs=3, use_hash=False):\n",
    "        \n",
    "        self.embedding_vector_length = embedding_vector_length\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.lstm_layer_sizes = lstm_layers\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.use_hashing_trick = use_hash\n",
    "        if not self.use_hashing_trick:\n",
    "            self.tokenizer = Tokenizer()\n",
    "            \n",
    "        \n",
    "    def _text_to_int_sequence(self, text):\n",
    "        return [self.tokenizer.word_index[word] for word in text_to_word_sequence(text)]\n",
    "        \n",
    "    def fit(self, X, y, validation_data):\n",
    "        \n",
    "        all_X = pd.concat([X, validation_data[0]])\n",
    "        if self.use_hashing_trick:\n",
    "            all_words = set()\n",
    "            for text in all_X:\n",
    "                new_words = set(text_to_word_sequence(text))\n",
    "                all_words = all_words.union(new_words)\n",
    "            self.max_vocab = len(all_words)*1.3\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                X[i] = hashing_trick(X[i], max_vocab, hash_function='md5')\n",
    "            X_pad = sequence.pad_sequences(X, maxlen=self.max_seq_length)\n",
    "            \n",
    "            X_valid = validation_data[0]\n",
    "            \n",
    "            for i in range(len(X_valid)):\n",
    "                X_valid[i] = hashing_trick(X_valid[i], max_vocab, hash_function='md5')\n",
    "            X_valid_pad = sequence.pad_sequences(X_valid, maxlen=self.max_seq_length)\n",
    "        \n",
    "            y_valid = validation_data[1]\n",
    "            \n",
    "        else:    \n",
    "            print('Fitting Tokenizer...')\n",
    "            self.tokenizer.fit_on_texts(all_X)\n",
    "            self.max_vocab = len(self.tokenizer.word_index) + 20\n",
    "            X = X.apply(self._text_to_int_sequence)\n",
    "            X_pad = sequence.pad_sequences(X, maxlen=self.max_seq_length)\n",
    "        \n",
    "            X_valid = validation_data[0].apply(self._text_to_int_sequence)\n",
    "            X_valid_pad = sequence.pad_sequences(X_valid, maxlen=self.max_seq_length)\n",
    "        \n",
    "            y_valid = validation_data[1]\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(self.max_vocab, self.embedding_vector_length, input_length=self.max_seq_length))\n",
    "        for lstm_layer_size in self.lstm_layer_sizes:\n",
    "            self.model.add(LSTM(lstm_layer_size))\n",
    "        \n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        early_stopping = EarlyStopping(monitor='loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=0, mode='min')\n",
    "        callbacks_list = [early_stopping]\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        print('Fitting model...')\n",
    "        self.model.fit(X_pad, y, validation_data=(X_valid_pad, y_valid), \n",
    "                  epochs=self.num_epochs, batch_size=self.batch_size, callbacks=callbacks_list)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            X = X.apply(self._text_to_int_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "        elif type(X) == str:\n",
    "            X = self._text_to_int_sequence(X)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "        else:\n",
    "            X = map(X, self.text_to_word_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        if type(X) == pd.core.series.Series:\n",
    "            X = X.apply(self._text_to_int_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "        elif type(X) == str:\n",
    "            X = self._text_to_int_sequence(X)\n",
    "            X = sequence.pad_sequence(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            X = map(X, self._text_to_word_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \n",
    "        pred = self.predict(X)\n",
    "        return accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Tokenizer...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 64)           6650432   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 6,716,533\n",
      "Trainable params: 6,716,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 457s - loss: 0.4486 - acc: 0.7871 - val_loss: 0.3822 - val_acc: 0.8335\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 458s - loss: 0.2219 - acc: 0.9158 - val_loss: 0.3171 - val_acc: 0.8690\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier = LSTM_Sentiment_Classifier(embedding_vector_length=64, max_seq_length=500, \n",
    "                                            lstm_layers=[100], num_epochs=2, use_hash=False)\n",
    "lstm_classifier.fit(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 64)           6649792   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 6,715,893\n",
      "Trainable params: 6,715,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 453s - loss: 0.4263 - acc: 0.8012 - val_loss: 0.3069 - val_acc: 0.8719\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 463s - loss: 0.2106 - acc: 0.9221 - val_loss: 0.2971 - val_acc: 0.8866\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 472s - loss: 0.1362 - acc: 0.9518 - val_loss: 0.3420 - val_acc: 0.8747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b43cba8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector_length = 64\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 10, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')\n",
    "callbacks_list = [early_stopping]\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=32, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
