{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "//anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Original Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('stanford_movie_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = data['review'].iloc[0:25000]\n",
    "y_train = data['sentiment'].iloc[0:25000]\n",
    "\n",
    "X_test = data['review'].iloc[25000:]\n",
    "y_test = data['sentiment'].iloc[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(imdb.get_word_index().values())) == len(imdb.get_word_index().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re # regex library\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # Effectively removes HTML markup tags\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['review'] = data['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103893"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_int_sequence(text):\n",
    "    return [tokenizer.word_index[word] for word in text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class ValAccCheck(Callback):\n",
    "    \n",
    "    def __init__(self, validation_data, interval=25):\n",
    "        self.validation_data = validation_data\n",
    "        self.val_accs = []\n",
    "        self.val_losses = []\n",
    "        self.interval = interval\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.steps_completed = 0\n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        \n",
    "        self.steps_completed += 1\n",
    "        if self.steps_completed % self.interval == 0:\n",
    "            x_valid, y_valid = self.validation_data[0], self.validation_data[1]\n",
    "            loss, acc = self.model.evaluate(x_valid, y_valid, verbose=0)\n",
    "            self.val_accs.append(acc)\n",
    "            self.val_losses.append(loss)\n",
    "            print('\\n')\n",
    "            print('val_acc: {0}, val_loss: {1}'.format(acc, loss))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "class LSTM_Sentiment_Classifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, embedding_vector_length, max_seq_length, lstm_layers, batch_size=32, num_epochs=3, use_hash=False,\n",
    "                dropout=None, conv_params=None):\n",
    "        \n",
    "        self.embedding_vector_length = embedding_vector_length\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.lstm_layer_sizes = lstm_layers\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.use_hashing_trick = use_hash\n",
    "        if not self.use_hashing_trick:\n",
    "            self.tokenizer = Tokenizer()\n",
    "        self.dropout = dropout\n",
    "        self.conv_params = conv_params\n",
    "        \n",
    "    def _text_to_int_sequence(self, text):\n",
    "        return [self.tokenizer.word_index[word] for word in text_to_word_sequence(text)]\n",
    "        \n",
    "    def fit(self, X, y, validation_data):\n",
    "        \n",
    "        all_X = pd.concat([X, validation_data[0]])\n",
    "        if self.use_hashing_trick:\n",
    "            all_words = set()\n",
    "            for text in all_X:\n",
    "                new_words = set(text_to_word_sequence(text))\n",
    "                all_words = all_words.union(new_words)\n",
    "            self.max_vocab = len(all_words)*1.3\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                X[i] = hashing_trick(X[i], max_vocab, hash_function='md5')\n",
    "            X_pad = sequence.pad_sequences(X, maxlen=self.max_seq_length)\n",
    "            \n",
    "            X_valid = validation_data[0]\n",
    "            \n",
    "            for i in range(len(X_valid)):\n",
    "                X_valid[i] = hashing_trick(X_valid[i], max_vocab, hash_function='md5')\n",
    "            X_valid_pad = sequence.pad_sequences(X_valid, maxlen=self.max_seq_length)\n",
    "        \n",
    "            y_valid = validation_data[1]\n",
    "            \n",
    "        else:    \n",
    "            print('Fitting Tokenizer...')\n",
    "            self.tokenizer.fit_on_texts(all_X)\n",
    "            self.max_vocab = len(self.tokenizer.word_index) + 20\n",
    "            X = X.apply(self._text_to_int_sequence)\n",
    "            X_pad = sequence.pad_sequences(X, maxlen=self.max_seq_length)\n",
    "        \n",
    "            X_valid = validation_data[0].apply(self._text_to_int_sequence)\n",
    "            X_valid_pad = sequence.pad_sequences(X_valid, maxlen=self.max_seq_length)\n",
    "        \n",
    "            y_valid = validation_data[1]\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(self.max_vocab, self.embedding_vector_length, input_length=self.max_seq_length))\n",
    "        if self.dropout is not None:\n",
    "            self.model.add(Dropout(self.dropout))\n",
    "            \n",
    "        if self.conv_params is not None:\n",
    "            self.model.add(Conv1D(filters=self.conv_params['filters'], \n",
    "                                  kernel_size=self.conv_params['kernel_size'], padding='same', activation='relu'))\n",
    "            self.model.add(MaxPooling1D(pool_size=self.conv_params['pool_size']))\n",
    "            \n",
    "            self.model.add(Conv1D(filters=2*self.conv_params['filters'], \n",
    "                                  kernel_size=self.conv_params['kernel_size'], padding='same', activation='relu'))\n",
    "            self.model.add(MaxPooling1D(pool_size=self.conv_params['pool_size']))\n",
    "            \n",
    "        if len(self.lstm_layer_sizes) > 1:\n",
    "            for lstm_layer_size in self.lstm_layer_sizes[:-1]:\n",
    "                self.model.add(LSTM(lstm_layer_size, return_sequences=True))\n",
    "                self.model.add(Dropout(self.dropout))\n",
    "            self.model.add(LSTM(self.lstm_layer_sizes[-1]))\n",
    "        else:\n",
    "            self.model.add(LSTM(self.lstm_layer_sizes[0]))\n",
    "        if self.dropout is not None:\n",
    "            self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=1,\n",
    "                              verbose=0, mode='auto')\n",
    "        val_acc_check = ValAccCheck((X_valid_pad, y_valid), interval=200)\n",
    "        callbacks_list = [early_stopping]\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        print('Fitting model...')\n",
    "        self.model.fit(X_pad, y, validation_data=(X_valid_pad, y_valid), \n",
    "                  epochs=self.num_epochs, batch_size=self.batch_size, callbacks=callbacks_list)\n",
    "        \n",
    "        self.val_loss_history = val_acc_check.val_losses\n",
    "        self.val_acc_history = val_acc_check.val_accs\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            X = X.apply(self._text_to_int_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "        elif type(X) == str:\n",
    "            X = self._text_to_int_sequence(X)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "        else:\n",
    "            X = map(X, self._text_to_int_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        if type(X) == pd.core.series.Series:\n",
    "            X = X.apply(self._text_to_int_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "        elif type(X) == str:\n",
    "            X = self._text_to_int_sequence(X)\n",
    "            X = sequence.pad_sequence(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            X = map(X, self._text_to_word_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \n",
    "        pred = self.predict(X)\n",
    "        return accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Tokenizer...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 500, 32)           3325216   \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 250, 64)           6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 125, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 125, 128)          98816     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,565,057\n",
      "Trainable params: 3,565,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 287s - loss: 0.3972 - acc: 0.8065 - val_loss: 0.2539 - val_acc: 0.8984\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 7888s - loss: 0.1617 - acc: 0.9430 - val_loss: 0.2858 - val_acc: 0.9012\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier = LSTM_Sentiment_Classifier(embedding_vector_length=32, max_seq_length=500, dropout=0.2, \n",
    "                                            lstm_layers=[128,128], num_epochs=2, use_hash=False,\n",
    "                                           conv_params={'filters':32, 'kernel_size':3, 'pool_size':2})\n",
    "lstm_classifier.fit(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very Deep CNNs for Text Classification (VDCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.engine import Layer, InputSpec\n",
    "from keras.layers import Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        \n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "        \n",
    "        # return flattened output\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "class VDCNN_Sentiment_Classifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, n_classes, embedding_vector_length, max_seq_length, batch_size=32, num_epochs=3, dropout=0.2):\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.embedding_vector_length = embedding_vector_length\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.dropout = dropout\n",
    "     \n",
    "    def _text_to_int_sequence(self, text):\n",
    "        return [self.tokenizer.word_index[word] for word in text_to_word_sequence(text)]\n",
    "    \n",
    "    def _create_convolutional_block(self, num_filters, kernel_size=3):\n",
    "        \n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=kernel_size,\n",
    "                             padding='same', activation='relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=kernel_size,\n",
    "                             padding='same', activation='relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, validation_data):\n",
    "        \n",
    "        all_X = pd.concat([X, validation_data[0]])\n",
    "        print('Fitting Tokenizer...')\n",
    "        self.tokenizer.fit_on_texts(all_X)\n",
    "        self.max_vocab = len(self.tokenizer.word_index) + 20\n",
    "        X = X.apply(self._text_to_int_sequence)\n",
    "        X_pad = sequence.pad_sequences(X, maxlen=self.max_seq_length)\n",
    "        \n",
    "        X_valid = validation_data[0].apply(self._text_to_int_sequence)\n",
    "        X_valid_pad = sequence.pad_sequences(X_valid, maxlen=self.max_seq_length)\n",
    "        \n",
    "        y_valid = validation_data[1]\n",
    "        #y = to_categorical(y)\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(self.max_vocab, self.embedding_vector_length, input_length=self.max_seq_length))\n",
    "        self.model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation=None))\n",
    "        \n",
    "        filter_sizes = [64, 128, 256, 512]\n",
    "        \n",
    "        for filter_size in filter_sizes:\n",
    "            self._create_convolutional_block(filter_size)\n",
    "            self._create_convolutional_block(filter_size)\n",
    "            if filter_size != 512:\n",
    "                self.model.add(MaxPooling1D(pool_size=3, strides=2))\n",
    "        \n",
    "        self.model.add(KMaxPooling(k=8))\n",
    "        self.model.add(Dense(4096, activation='relu'))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(2048, activation='relu'))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(2048, activation='relu'))\n",
    "        self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=False)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=1,\n",
    "                              verbose=0, mode='auto')\n",
    "        \n",
    "        callbacks_list = [early_stopping]\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        print('Fitting model...')\n",
    "        self.model.fit(X_pad, y, validation_data=(X_valid_pad, y_valid), \n",
    "                  epochs=self.num_epochs, batch_size=self.batch_size, callbacks=callbacks_list)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            X = X.apply(self._text_to_int_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "        elif type(X) == str:\n",
    "            X = self._text_to_int_sequence(X)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "        else:\n",
    "            X = map(X, self._text_to_int_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        if type(X) == pd.core.series.Series:\n",
    "            X = X.apply(self._text_to_int_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "        elif type(X) == str:\n",
    "            X = self._text_to_int_sequence(X)\n",
    "            X = sequence.pad_sequence(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            X = map(X, self._text_to_word_sequence)\n",
    "            X = sequence.pad_sequences(X, maxlen = self.max_seq_length)\n",
    "            return self.model.predict_proba(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \n",
    "        pred = self.predict(X)\n",
    "        return accuracy_score(y, pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Tokenizer...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 32)           3325216   \n",
      "_________________________________________________________________\n",
      "conv1d_172 (Conv1D)          (None, 500, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_173 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_161 (Bat (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_174 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_162 (Bat (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_175 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_163 (Bat (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_176 (Conv1D)          (None, 500, 64)           12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_164 (Bat (None, 500, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_177 (Conv1D)          (None, 249, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_165 (Bat (None, 249, 128)          512       \n",
      "_________________________________________________________________\n",
      "conv1d_178 (Conv1D)          (None, 249, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_166 (Bat (None, 249, 128)          512       \n",
      "_________________________________________________________________\n",
      "conv1d_179 (Conv1D)          (None, 249, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_167 (Bat (None, 249, 128)          512       \n",
      "_________________________________________________________________\n",
      "conv1d_180 (Conv1D)          (None, 249, 128)          49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_168 (Bat (None, 249, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 124, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 124, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_169 (Bat (None, 124, 256)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_182 (Conv1D)          (None, 124, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_170 (Bat (None, 124, 256)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 124, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_171 (Bat (None, 124, 256)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 124, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_172 (Bat (None, 124, 256)          1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 61, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 61, 512)           393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_173 (Bat (None, 61, 512)           2048      \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 61, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_174 (Bat (None, 61, 512)           2048      \n",
      "_________________________________________________________________\n",
      "conv1d_187 (Conv1D)          (None, 61, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_175 (Bat (None, 61, 512)           2048      \n",
      "_________________________________________________________________\n",
      "conv1d_188 (Conv1D)          (None, 61, 512)           786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_176 (Bat (None, 61, 512)           2048      \n",
      "_________________________________________________________________\n",
      "k_max_pooling_10 (KMaxPoolin (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 36,382,817\n",
      "Trainable params: 36,375,137\n",
      "Non-trainable params: 7,680\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 1055s - loss: 0.7368 - acc: 0.5194 - val_loss: 0.8909 - val_acc: 0.5016\n",
      "Epoch 2/3\n",
      "  992/25000 [>.............................] - ETA: 873s - loss: 0.6733 - acc: 0.5756"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-17247fff21bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m vdcnn_classifier = VDCNN_Sentiment_Classifier(n_classes=2, embedding_vector_length=32, \n\u001b[1;32m      2\u001b[0m                                               max_seq_length=500, num_epochs=3, batch_size=32)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvdcnn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-e292dac15046>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, validation_data)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         self.model.fit(X_pad, y, validation_data=(X_valid_pad, y_valid), \n\u001b[0;32m---> 80\u001b[0;31m                   epochs=self.num_epochs, batch_size=self.batch_size, callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amol/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amol/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amol/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amol/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amol/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amol/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vdcnn_classifier = VDCNN_Sentiment_Classifier(n_classes=2, embedding_vector_length=32, \n",
    "                                              max_seq_length=500, num_epochs=3, batch_size=32)\n",
    "vdcnn_classifier.fit(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Multi-Layered Perceptron with Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 117.02909994125366 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import time\n",
    "max_feats = 10000\n",
    "tfidf = TfidfVectorizer(strip_accents=None, \n",
    "                        lowercase=False,\n",
    "                        preprocessor=preprocessor,\n",
    "                        min_df=2,\n",
    "                        max_features=max_feats,\n",
    "                        ngram_range=(1,2))\n",
    "\n",
    "mlp_pipeline = Pipeline([('tfidf', tfidf), ('mlp', MLPClassifier(hidden_layer_sizes=[1000,1000,1000],\n",
    "                                                                max_iter=1000,\n",
    "                                                                activation='logistic'))])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print('Training model...')\n",
    "mlp_pipeline.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed: {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "pred = mlp_pipeline.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
